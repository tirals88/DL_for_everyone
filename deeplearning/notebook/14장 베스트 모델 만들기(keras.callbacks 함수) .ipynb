{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mSvnn-ZWd9ef",
        "UhcQBmxwO5Vm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/DL_for_everyone/blob/main/deeplearning/notebook/14%EC%9E%A5%20%EB%B2%A0%EC%8A%A4%ED%8A%B8%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0(keras.callbacks%20%ED%95%A8%EC%88%98)%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'모두의 딥러닝' 책 스터디 내용을 jupyter notebook으로 정리하여 올립니다.\n",
        "\n",
        "Github 주소 : 'https://github.com/gilbutITbook/080228'\n",
        "\n",
        "**모두의 딥러닝**"
      ],
      "metadata": {
        "id": "WdeE-kzODzDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14 장 베스트 모델 만들기\n",
        "\n",
        "이번 장에서는 레드와인과 화이트와인을 구분하는 모델을 볼 것이다.\n",
        "데이터는 레드와인 샘플 1599개를 등급과 맛, 산도를 측정해 분석하고, 화이트와인 샘플 4898개를 마찬가지로 분석해 하나로 합친 것이다."
      ],
      "metadata": {
        "id": "mSvnn-ZWd9ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인\n",
        "import pandas as pd\n",
        "\n",
        "df_pre = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080228/master/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac=1)"
      ],
      "metadata": {
        "id": "eKROL3263FGD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample 함수는 원본 데이터에서 정해진 비율만큼 랜덤으로 뽑아오는 함수이다.\n",
        "\n",
        "frac = 1 이라고 지정하면 원본 데이터의 100%를 랜덤으로 불러오라는 의미이다.\n",
        "\n",
        "frac = .5 로 지정하면 50%만 랜덤으로 불러온다."
      ],
      "metadata": {
        "id": "9tIrmU5J76BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#랜덤으로 모두 불러온 원본 데이터 처음 5줄 출력\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "GP2f8dFf8Mze",
        "outputId": "c2e95d4a-f034-43e8-f0b8-d2d3ba22f494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0      1     2    3      4     5      6        7     8     9     10  \\\n",
            "5163  6.6  0.640  0.28  4.4  0.032  19.0   78.0  0.99036  3.11  0.62  12.9   \n",
            "463   8.1  0.660  0.70  2.2  0.098  25.0  129.0  0.99720  3.08  0.53   9.0   \n",
            "1040  7.4  0.965  0.00  2.2  0.088  16.0   32.0  0.99756  3.58  0.67  10.2   \n",
            "85    6.9  0.550  0.15  2.2  0.076  19.0   40.0  0.99610  3.41  0.59  10.1   \n",
            "6465  5.7  0.410  0.21  1.9  0.048  30.0  112.0  0.99138  3.29  0.55  11.2   \n",
            "\n",
            "      11  12  \n",
            "5163   6   0  \n",
            "463    5   1  \n",
            "1040   5   1  \n",
            "85     5   1  \n",
            "6465   6   0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한 줄당 모두 13개의 정보가 있는 것을 확인할 수 있다.\n"
      ],
      "metadata": {
        "id": "QYmBX5kCtjJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 정보 출력\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "Cb_I5ZmD8fR0",
        "outputId": "687a97e8-d58c-4145-b488-05fd041234df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6497 entries, 5163 to 3719\n",
            "Data columns (total 13 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       6497 non-null   float64\n",
            " 1   1       6497 non-null   float64\n",
            " 2   2       6497 non-null   float64\n",
            " 3   3       6497 non-null   float64\n",
            " 4   4       6497 non-null   float64\n",
            " 5   5       6497 non-null   float64\n",
            " 6   6       6497 non-null   float64\n",
            " 7   7       6497 non-null   float64\n",
            " 8   8       6497 non-null   float64\n",
            " 9   9       6497 non-null   float64\n",
            " 10  10      6497 non-null   float64\n",
            " 11  11      6497 non-null   int64  \n",
            " 12  12      6497 non-null   int64  \n",
            "dtypes: float64(11), int64(2)\n",
            "memory usage: 710.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "처음 12개의 데이터는 와인의 pH, 알코올 도수, 염화나트륨 농도, 맛(~10 등급) 등의 정보이고, 13번째 데이터 'class' 는 1 : 레드와인, 0 : 화이트와인을 나타낸다."
      ],
      "metadata": {
        "id": "SKhsXyts8s7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]"
      ],
      "metadata": {
        "id": "bHoApU019BAR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#seed 값 설정\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "#모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "#모델 실행\n",
        "model.fit(X, Y, epochs=200, batch_size=200)\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "metadata": {
        "id": "E1oHkJuqAq_z",
        "outputId": "cc917b1b-8005-44f1-f368-3732240d0d0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 3ms/step - loss: 26.9945 - accuracy: 0.2461\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 3.2940 - accuracy: 0.5723\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.9098\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.9230\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.9283\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.9344\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9366\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9387\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9409\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1713 - accuracy: 0.9417\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1651 - accuracy: 0.9431\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9443\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9458\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1482 - accuracy: 0.9469\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9489\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9486\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9531\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9529\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9535\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1212 - accuracy: 0.9543\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9560\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.9567\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.9589\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 0.9609\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9600\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1061 - accuracy: 0.9634\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1056 - accuracy: 0.9637\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9643\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1015 - accuracy: 0.9644\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0993 - accuracy: 0.9668\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9688\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9678\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9683\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.9697\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9715\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9709\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9731\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9718\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9737\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.9740\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.9735\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9743\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9748\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9760\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9741\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.9748\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9766\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9751\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9758\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9749\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9768\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9752\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9769\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9769\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9786\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9771\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9749\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9789\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9797\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9781\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9772\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9786\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9788\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9761\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9795\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9780\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9766\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9805\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9801\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9794\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9797\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.9805\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0670 - accuracy: 0.9798\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9794\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9806\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9797\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9808\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9792\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0608 - accuracy: 0.9811\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9803\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9800\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9817\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9823\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9829\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9814\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9817\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9821\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9812\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9803\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9801\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9821\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9806\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9823\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9832\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9831\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9828\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9817\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9829\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 0.9834\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9826\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9820\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9828\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9825\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9826\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9815\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9829\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9835\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9812\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9840\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9838\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9840\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9826\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9841\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9838\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9835\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9825\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9825\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9843\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9808\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9841\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9846\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9852\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9825\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9841\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9841\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9855\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9845\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9848\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9837\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9841\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9851\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9838\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9838\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9840\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9841\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9838\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9848\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0530 - accuracy: 0.9852\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9851\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9854\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9852\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9854\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9843\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9829\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9838\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9852\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9851\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9855\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9845\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9838\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9858\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9846\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9854\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9854\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9848\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9854\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9843\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9823\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9851\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9857\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9831\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9860\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9861\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9843\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9861\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9865\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9841\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9838\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9840\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9860\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9866\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9852\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9868\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9852\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9845\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9846\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9857\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9845\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9852\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9855\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9848\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9849\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9851\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0503 - accuracy: 0.9863\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9855\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9841\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9855\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9858\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9861\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9846\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9835\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9845\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9829\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9855\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9849\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9818\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9841\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9863\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9854\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0458 - accuracy: 0.9868\n",
            "\n",
            " Accuracy: 0.9868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 저장과 재사용은 이전 장에서 다음과 같이 배웠다\n",
        "```\n",
        "import keras.models\n",
        "#앞서 학습한 모델을 저장\n",
        "model.save('my_model.h5')\n",
        "\n",
        "#모델을 다시 불러옴\n",
        "model = keras.models.load_model('/content/my_model.h5')\n",
        "\n",
        "```\n",
        "이번에는 에포크마다 모델의 정확도를 함께 기록하면서 저장을 해볼 것이다.\n",
        "\n",
        "먼저 폴더를 지정하고 파일이름은 hdf5라는 확장자로 저장한다.</br>그리고 에포크 횟수와 테스트셋 오차 값을 이용하여 파일 이름을 만드는데 100번째 에포크에 오차가 0.0612라면 파일명은 **100-0.0612.hdf5**가 된다.\n",
        "\n",
        "```\n",
        "import os\n",
        "\n",
        "MODEL_DIR = './model/'              #모델을 저장하는 폴더\n",
        "if not os.path.exists(MODEL_DIR):   #해당 위치에 폴더가 존재하지 않으면\n",
        "  os.mkdir(MODEL_DIR)               #폴더 생성\n",
        "\n",
        "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "```"
      ],
      "metadata": {
        "id": "sjePGfkd3jva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델을 저장하기 위해 케라스의 콜백 함수 중 ModelCheckpoint() 함수를 불러온다.\n",
        "\n",
        "그리고 checkpointer라는 변수를 만들어 모니터할 값을 지정한다.\n",
        "\n",
        "(케라스 내부에서 학습 정확도 : 'acc', 학습셋 오차 : 'loss', 테스트셋 정확도 : 'val_acc', 테스트셋 오차 : 'val_loss')\n",
        "\n",
        "모델이 저장될 곳을 앞서 만든 modelpath로 지정하고 verbose의 값을 1로 정하면 해당 함수의 진행 사항이 출력되고, 0으로 정하면 출력되지 않는다.\n",
        "\n",
        "이제 모델을 학습할 때마다 위에서 정한 checkpointer의 값을 받아 지정된 곳에 모델을 저장한다.\n",
        "```\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss, verbose=1)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])\n",
        "```"
      ],
      "metadata": {
        "id": "AHA4TuLa5OYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MODEL_DIR = './model/'              #모델을 저장하는 폴더\n",
        "if not os.path.exists(MODEL_DIR):   #해당 위치에 폴더가 존재하지 않으면\n",
        "  os.mkdir(MODEL_DIR)               #폴더 생성\n",
        "\n",
        "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])"
      ],
      "metadata": {
        "id": "a6uB_vQw2EOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력된 코드 일부\n",
        "```\n",
        "Epoch 196: saving model to ./model/196-0.0446.hdf5\n",
        "\n",
        "Epoch 197: saving model to ./model/197-0.0464.hdf5\n",
        "\n",
        "Epoch 198: saving model to ./model/198-0.0532.hdf5\n",
        "\n",
        "Epoch 199: saving model to ./model/199-0.0459.hdf5\n",
        "\n",
        "Epoch 200: saving model to ./model/200-0.0474.hdf5\n",
        "```"
      ],
      "metadata": {
        "id": "AtBhH25hMg6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#앞서 저장한 모델보다 나아졌을 때만 저장\n",
        "checkpointer.save_best_only = True\n",
        "\n",
        "#checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "W7bet9dP7QGl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#수정된 전체 코드\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#seed 값 설정\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "#모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "#모델 저장 폴더 설정\n",
        "MODEL_DIR = './re_model/'              #모델을 저장하는 폴더\n",
        "if not os.path.exists(MODEL_DIR):      #해당 위치에 폴더가 존재하지 않으면\n",
        "  os.mkdir(MODEL_DIR)                  #폴더 생성\n",
        "\n",
        "#모델 저장 조건 설정\n",
        "modelpath=\"./re_model/{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, moniter='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "#모델 실행\n",
        "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])\n",
        "\n",
        "#print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "metadata": {
        "id": "QIBbPV9b8DRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력된 코드 일부\n",
        "```\n",
        "Epoch 177: val_loss did not improve from 0.04985\n",
        "\n",
        "Epoch 178: val_loss did not improve from 0.04985\n",
        "\n",
        "Epoch 179: val_loss improved from 0.04985 to 0.04979, saving model to ./re_model/179-0.0498.hdf5\n",
        "\n",
        "Epoch 180: val_loss did not improve from 0.04979\n",
        "\n",
        "Epoch 181: val_loss did not improve from 0.04979\n",
        "\n",
        "Epoch 182: val_loss improved from 0.04979 to 0.04969, saving model to ./re_model/182-0.0497.hdf5\n",
        "\n",
        "Epoch 183: val_loss did not improve from 0.04969\n",
        "\n",
        "Epoch 184: val_loss did not improve from 0.04969\n",
        "```"
      ],
      "metadata": {
        "id": "EjJtK5c6MUSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 딥러닝 프레임워크가 만들어 낸 모델을 업데이트하는 과정을 알아볼 것이다.\n",
        "\n",
        "먼저 에포크를 얼마나 지정할지를 결정해야 한다.\n",
        "\n",
        "모델의 학습 시간에 따른 정확도와 테스트 결과를 그래프를 통해 확인해 볼 것이다.\n",
        "\n",
        "```\n",
        "df = df_pre.sample(frac=0.15)\n",
        "\n",
        "history=model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)\n",
        "```\n",
        "모델이 학습되는 과정을 history변수를 만들어 저장하고 긴 학습의 예를 지켜보기 위해 에포크를 3500으로 조정하고 시간을 오래걸리지 않게 하기 위해 sample() 함수를 통해 15%의 데이터만들을 불러온다.\n",
        "\n",
        "배치 크기는 500으로 하여 한 번 가동할 때 더 많은 입력을 받게끔 하였다.\n",
        "\n",
        "\n",
        "다음으로 그래프로 표현하기 위해 오차와 정확도 값을 정하여 저장한다.\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_vloss=history.history['val_loss']\n",
        "y_acc=history.history['acc']\n",
        "```\n",
        "\n",
        "x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시해본다.\n",
        "\n",
        "```\n",
        "x_len = np.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "```"
      ],
      "metadata": {
        "id": "_EW696uFIzbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 코드\n",
        "\n",
        "#수정된 전체 코드\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#seed 값 설정\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080228/master/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac=0.15)\n",
        "\n",
        "dataset=df.values\n",
        "X = dataset[:, :12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "#모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "#모델 저장 폴더 설정\n",
        "MODEL_DIR = './re_model/'              #모델을 저장하는 폴더\n",
        "if not os.path.exists(MODEL_DIR):      #해당 위치에 폴더가 존재하지 않으면\n",
        "  os.mkdir(MODEL_DIR)                  #폴더 생성\n",
        "\n",
        "#모델 저장 조건 설정\n",
        "modelpath=\"./re_model/{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, moniter='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "#모델 실행 및 저장\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)\n",
        "\n",
        "#print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "metadata": {
        "id": "axbIpc6qKX7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 코드 일부\n",
        "\n",
        "Epoch : 1000번대\n",
        "```\n",
        "Epoch 1002/3500\n",
        "2/2 [==============================] - 0s 86ms/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 0.0754 - val_accuracy: 0.9752\n",
        "Epoch 1003/3500\n",
        "2/2 [==============================] - 0s 58ms/step - loss: 0.0302 - accuracy: 0.9877 - val_loss: 0.0721 - val_accuracy: 0.9814\n",
        "Epoch 1004/3500\n",
        "2/2 [==============================] - 0s 63ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.0720 - val_accuracy: 0.9814\n",
        "Epoch 1005/3500\n",
        "2/2 [==============================] - 0s 73ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.0719 - val_accuracy: 0.9814\n",
        "Epoch 1006/3500\n",
        "2/2 [==============================] - 0s 55ms/step - loss: 0.0271 - accuracy: 0.9893 - val_loss: 0.0736 - val_accuracy: 0.9814\n",
        "```\n",
        "Epoch : 3500 대\n",
        "\n",
        "```\n",
        "Epoch 3496/3500\n",
        "2/2 [==============================] - 0s 70ms/step - loss: 0.0018 - accuracy: 0.9985 - val_loss: 0.1746 - val_accuracy: 0.9783\n",
        "Epoch 3497/3500\n",
        "2/2 [==============================] - 0s 56ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1773 - val_accuracy: 0.9783\n",
        "Epoch 3498/3500\n",
        "2/2 [==============================] - 0s 62ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1814 - val_accuracy: 0.9783\n",
        "Epoch 3499/3500\n",
        "2/2 [==============================] - 0s 72ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1827 - val_accuracy: 0.9783\n",
        "Epoch 3500/3500\n",
        "2/2 [==============================] - 0s 50ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9783\n",
        "```\n",
        "\n",
        "train_set 의 정확도는 100%에 가까워지지만, val_loss 는 오히려 증가하여 과적합이 일어난 양상을 확인할 수 있다.\n"
      ],
      "metadata": {
        "id": "tridaooOPL2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss=history.history['val_loss']\n",
        "y_acc=history.history['accuracy']\n",
        "\n",
        "x_len = np.arange(len(y_acc))\n",
        "\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aw6NKxbvE4JU",
        "outputId": "14e21f12-4912-4b44-b8af-0ed97e829549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EarlyStopping\n",
        "\n",
        "이럴 떄 사용할 수 있는 함수로 케라스의 EarlyStopping()이 있다.\n",
        "\n",
        "EarlyStopping() 함수는 모니터할 값과 테스트 오차가 좋아지지 않아도 몇 번 까지 기다릴지를 정한다.\n",
        "\n",
        "이를 early_stopping_callback 에 저장한다.\n",
        "\n",
        "```\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.33, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "oi4-Ptm_QqY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EarlyStopping 도입 후 코드\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#seed 값 설정\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080228/master/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac=0.15)\n",
        "\n",
        "dataset=df.values\n",
        "X = dataset[:, :12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "#모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#학습 자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "#모델 실행\n",
        "history=model.fit(X, Y, validation_split=0.2, epochs=2000,\n",
        "                  batch_size=500, callbacks=[early_stopping_callback])\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" %(model.evaluate(X, Y)[1]))"
      ],
      "metadata": {
        "id": "FcAt8ctWQfez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 코드 마지막 부분\n",
        "```\n",
        "Epoch 1191/2000\n",
        "2/2 [==============================] - 0s 70ms/step - loss: 0.0212 - accuracy: 0.9910 - val_loss: 0.0599 - val_accuracy: 0.9846\n",
        "Epoch 1192/2000\n",
        "2/2 [==============================] - 0s 68ms/step - loss: 0.0250 - accuracy: 0.9897 - val_loss: 0.0590 - val_accuracy: 0.9846\n",
        "Epoch 1193/2000\n",
        "2/2 [==============================] - 0s 48ms/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.0580 - val_accuracy: 0.9897\n",
        "Epoch 1194/2000\n",
        "2/2 [==============================] - 0s 64ms/step - loss: 0.0254 - accuracy: 0.9897 - val_loss: 0.0621 - val_accuracy: 0.9846\n",
        "Epoch 1195/2000\n",
        "2/2 [==============================] - 0s 55ms/step - loss: 0.0230 - accuracy: 0.9872 - val_loss: 0.0647 - val_accuracy: 0.9846\n",
        "31/31 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.9908\n",
        "\n",
        " Accuracy: 0.9908\n",
        " ```"
      ],
      "metadata": {
        "id": "RKRULz5KW56N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "에포크는 2000 이었지만, 도중에 멈추는 것을 볼 수 있다.\n",
        "\n",
        "총 1195의 학습이 진행되었고 100번 전의 학습인 1095의 val_loss 가 0.0570으로 가장 작은 수임을 확인해볼 수 있다."
      ],
      "metadata": {
        "id": "gFtEFHknU0rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss=history.history['val_loss']\n",
        "y_acc=history.history['accuracy']\n",
        "\n",
        "x_len = np.arange(len(y_acc))\n",
        "\n",
        "min = np.min(y_vloss)\n",
        "\n",
        "print(\"min value of \\'val_loss\\': %.04f, and its index : %.04d\" % (min, y_vloss.index(min)))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s1pSK8t7O0u0",
        "outputId": "eb809f87-5bad-4ef6-f31c-00d0fe680a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min value of 'val_loss': 0.0570, and its index : 1094\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 위에서 시도했던 모델 업데이트 함수와 EarlyStopping 을 동시에 사용할 수 있다."
      ],
      "metadata": {
        "id": "_GSYlg5YW-SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 코드\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#seed 값 설정\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080228/master/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac=0.15)\n",
        "\n",
        "dataset=df.values\n",
        "X = dataset[:, :12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "#모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#모델 저장 폴더 생성 및 업데이트/저장\n",
        "MODEL_DIR = './final_model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "modelpath='./final_model/{epoch:.02f}-{val_loss:.4f}.hdf5'\n",
        "\n",
        "checkpointer=ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "#학습 자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "#모델 실행\n",
        "model.fit(X, Y, validation_split=0.2, epochs=3500, batch_size=500, verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" %(model.evaluate(X, Y)[1]))"
      ],
      "metadata": {
        "id": "Gft_MSfWUs01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 코드 일부\n",
        "```\n",
        "Epoch 607: val_loss did not improve from 0.07344\n",
        "\n",
        "Epoch 608: val_loss improved from 0.07344 to 0.07108, saving model to ./final_model/608.00-0.0711.hdf5\n",
        "\n",
        "Epoch 609: val_loss improved from 0.07108 to 0.07023, saving model to ./final_model/609.00-0.0702.hdf5\n",
        "\n",
        "Epoch 610: val_loss did not improve from 0.07023\n",
        "\n",
        "Epoch 611: val_loss did not improve from 0.07023\n",
        "\n",
        "Epoch 612: val_loss did not improve from 0.07023\n",
        "```\n",
        "```\n",
        "Epoch 706: val_loss did not improve from 0.07023\n",
        "\n",
        "Epoch 707: val_loss did not improve from 0.07023\n",
        "\n",
        "Epoch 708: val_loss did not improve from 0.07023\n",
        "\n",
        "Epoch 709: val_loss did not improve from 0.07023\n",
        "31/31 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9846\n",
        "\n",
        " Accuracy: 0.9846\n",
        " ```\n",
        " 609번째 학습에서 마지막 갱신 및 저장이 이루어지고 이후 100번의 학습 동안 val_loss 의 갱신이 이루어지지 않아 학습이 중단된 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "St8gcAG5apx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "크게 총 두가지의 keras 콜백 함수를 배웠다.\n",
        "- ModelCheckpoint : 학습이 진행될 때 마다 모델을 저장할 수 있게 해주며, **'save_best_only=True'**를 통해 모니터하는 값이 갱신이 될 때만 저장하도록 설정할 수 있다.\n",
        "\n",
        "- EarlyStopping : 모니터하는 값이 **patience=number**로 정한 만큼 갱신되지 않을 때 그 이상 과적합을 방지하기 위해 학습을 자동 중단시켜준다."
      ],
      "metadata": {
        "id": "HGTdOk3sX8DV"
      }
    }
  ]
}