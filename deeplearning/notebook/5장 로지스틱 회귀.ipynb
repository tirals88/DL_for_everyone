{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mSvnn-ZWd9ef",
        "UhcQBmxwO5Vm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/DL_for_everyone/blob/main/deeplearning/notebook/5%EC%9E%A5%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'모두의 딥러닝' 책 스터디 내용을 jupyter notebook으로 정리하여 올립니다.\n",
        "\n",
        "Github 주소 : 'https://github.com/gilbutITbook/080228'\n",
        "\n",
        "**모두의 딥러닝**"
      ],
      "metadata": {
        "id": "WdeE-kzODzDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05 장 로지스틱 회귀 / 06 장\n",
        "\n",
        "05장 로지스틱 회귀 logistic regression는 '참', '거짓'을 구분하는 과정이며 이 부분은 이전에 했던 'Deep Learning for Scratch' [스터디](https://github.com/tirals88/Deep-Learning-from-Scratch/blob/06a51972011ef6082c252b0a5c9b96c9f0cd382b/1%EA%B6%8C_2%EC%9E%A5_%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0_+_3%EC%9E%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D(3%EC%B8%B5_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B9%8C%EC%A7%80).ipynb)와 같이 보았다.\n",
        "\n",
        "활성화 함수의 한 종류로서 계단 함수는 임계치를 기준으로 참과 거짓을 나눈다.</br>그리고 활성화 함수의 또 다른 예시로 시그모이드 Sigmoid 함수가 있다.\n",
        "\n",
        "이 둘의 차이로는 **'매끄러움'**이 있다. 시그모이드 함수는 연속적인 값을 출력하여 미분에 용이하다.\n",
        "\n",
        "또 다른 예시로 ReLU 함수는 임계치를 넘으면 그대로 출력을 하고, 넘지 못하면 0을 출력한다.\n",
        "\n",
        "\n",
        "시그모이드 함수는 $$y = \\frac{1}{1+e^{-(ax+b)}}$$ 로 표현될 수 있으며 결국 선형 회귀와 마찬가지로 $a 와 b$를 최적화 하는 것이다.\n",
        "\n",
        "기울기 $a$는 시그모이드 함수의 경사를, $b$는 함수의 좌우 이동에 관여한다.\n",
        "\n",
        "시그모이드 함수의 $a$에 대한 변화량(편미분)은 $log{x}$ 의 꼴을, 오차는 반대로 $-log{x}$의 꼴을 보인다.\n",
        "\n",
        "그리고 오차의 $b$에 대한 변화량은 이차곡선 꼴을 보인다.\n",
        "\n",
        "오차는 정답이 1일 때, 그리고 정답이 0일 때로 나눌 수 있다.\n",
        "\n",
        "두 경우 오차의 계산식은 $1 - \\frac{1}{1+e^{-(ax+b)}}$ = $\\frac{e^{-(ax+b)}}{1+e^{-(ax+b)}}$\n",
        "\n",
        "'https://knowledgeforengineers.tistory.com/86?category=946640' 이후 다시 공부 예정"
      ],
      "metadata": {
        "id": "mSvnn-ZWd9ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .."
      ],
      "metadata": {
        "id": "BPzVzOrItn3t"
      }
    }
  ]
}